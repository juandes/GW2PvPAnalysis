---
title: "Guild Wars 2 PvP Data Analysis"
author: "Juande"
output: html_document
---

```{r, echo=FALSE, include=FALSE}
require(ggplot2)
require(reshape2)
require(plyr)
require(dplyr)
require(randomForest)
require(e1071)
set.seed(3392)
```

```{r, echo=FALSE}
setwd("~/Development/R/GW2PvPAnalysis")
df <- read.csv("~/Development/R/GW2PvPAnalysis/GW2_PvP_data.csv", stringsAsFactors=FALSE)
```


```{r, echo=FALSE}
#Remove unnecessary columns
df$X <- NULL
df$X.1 <- NULL
df$X.2 <- NULL

#Rename columns
colnames(df) <- c("map", "red.score", "blue.score", "winner", "red_1", "red_2", 
                  "red_3", "red_4", "red_5", "blue_1", "blue_2", "blue_3", "blue_4",
                  "blue_5")

#If the red team won, the value is 0, otherwise 1
df$winner[df$red.score > df$blue.score] <- 0
df$winner[df$red.score < df$blue.score] <- 1

#Change the names of the classes for numbers
df[5:14][df[5:14] == "ele"] <- 1
df[5:14][df[5:14] == "engi"] <- 2
df[5:14][df[5:14] == "g"] <- 3
df[5:14][df[5:14] == "mes"] <- 4
df[5:14][df[5:14] == "necro"] <- 5
df[5:14][df[5:14] == "ran"] <- 6
df[5:14][df[5:14] == "t"] <- 7
df[5:14][df[5:14] == "war"] <- 8
```

# Introduction
***

The game Guild Wars 2, release on 2012, introduces a player versus player (PvP)
game mode called Conquest. In Conquest, there are two teams made of five players,
and the main objective is to seize capture points to increase your team score
(the more points you have, the faster the score increases); other ways to increase
the score if by killing players of the opposing team, non-player characters (NPC)
or by getting special bonuses.

This work is a data analysis done on a dataset made of 119 Conquest matches. In
the analysis we will take a look at several factors regarding a Conquest match,
such as the map where the match was played, the final score of both teams, the result
of the match and the composition of the teams.

# The dataset
***

As mentioned before, the dataset consists of 119 matches done during April and
May 2015 and in the following table we will show the first five observations
of the dataset.

```{r, echo=FALSE}
head(df)
```

The first column is the map where the match was played. The next two are the final
scores for the red and blue team, followed by the winner (0 means the red team won,
and 1 means blue won). The last ten columns are about the structure of the team in
terms of profession or classes. The first five colums represent the composition 
of the red team and the other five, the blue team.

For those of you interested, this is the codebook of what the number means.

* The maps: 1 is Battle of Kyhlo, 2 is Forest of Niflhel, 3 is Legacy of the Foefire
and 4 is for Temple of the Silent Storm.
* The classes: 0 is an empty slot (someone who never joined the match, was
disconnected, or quitted), 1 is Elementalist, 2 is Engineer, 3 is Guardian, 4 is
Mesmer, 5 is Necromancer, 6 is Ranger, 7 is Thief and 8 is Warrior.




# The maps
***

In the first section of this report, we took a look at the maps and the frequency
each one of them had of being selected. The next table shows the frequency and
percentage of usage of each map.

```{r,echo=FALSE}
# Create frequency table of map usage
map.frequency.percentage.table <- as.data.frame(table(df$map))
map.frequency.percentage.table <- cbind(map.frequency.percentage.table,
                                        sapply(as.data.frame(prop.table(table(df$map)))[,2],
                                                       function(x) round(x * 100, 2)))
colnames(map.frequency.percentage.table) <- c("Map", "Frequency", "Percentage")
rownames(map.frequency.percentage.table) <- c("Battle of Kyhlo", "Forest of Niflhel",
                                   "Legacy of the Foefire", "Temple of the Silent Storm")
map.frequency.percentage.table

# Histogram of map usage
qplot(map, data = df, geom = "histogram", main = "Histogram of map usage", xlab = "Map",
      ylab = "Frequency", fill = I("blue")) +
  scale_x_discrete(breaks = 1:4, labels = c("1" = "Kyhlo", "2" = "Niflhel", "3" = "Foefire",
                                            "4" = "Silent Storm"))
```

As seen by both the table and plot, the most selected map was Forest of Niflhel,
with a percentage of `r map.frequency.percentage.table[2, 3]`% and the least selected,
was Battle of Kyhlo with `r map.frequency.percentage.table[1, 3]`%.

##Score and result of the matches
***

This part of the analysis is related to the scores the result of the matches.
First, we will give an overview of the final scores and the outcome of the matches,
followed by an in depth view at the scores of the victor teams and the defeated
teams.

###Overall view of the score and result

We will start this section by displaying a table with the frequency of wins for
both teams.

```{r,echo=FALSE}
winnerdf <- as.data.frame(table(df$winner))
colnames(winnerdf) <- c("Team", "Frequency")
winnerdf[,1] <- c("Red team", "Blue team")
winnerdf
```

The red team won 49 of the matches or `r round(49/(49 + 70)*100, 2)`%, while the 
blue team won 70 or `r round(70/(49+70)*100, 2)`% of the matches. The percentages 
of victories per map follows this same pattern. In the map Battle of Kyhlo, the 
red team won `r round(7/(7+8)*100, 2)`% of the matches and the blue team won 
`r round(8/(7+8)*100, 2)`%, making this the only map where the blue team obtained
more victories than the red team. For the next map, Forest of Niflhel, the 
respective percentages were `r round(21/(21+32)*100, 2)`% and 
`r round(32/(21+32)*100, 2)`%, for Legacy of the Foefire `r round(11/(11+22)*100, 2)`% 
and for Temple of the Silent Storm, `r round(10/(10+8)*100, 2)`% and
`r round(8/(10+8)*100, 2)`%. This is shown in the next table.

```{r, echo=FALSE}
wins.per.map <- table(df$map, df$winner)

# Create data frame with the teams, their frequency of victories and percentage
wins.per.map <- data.frame(map = c("Battle of Kyhlo","Forest of Niflhel",
                                   "Legacy of the Foefire","Temple of the Silent Storm"),
                           red.team = wins.per.map[,1], blue.team = wins.per.map[,2],
                           stringsAsFactors=FALSE)
                                
wins.per.map.percentage <- wins.per.map
wins.per.map.percentage$red.team.win.percent <- round(wins.per.map$red.team / (wins.per.map$red.team +
                                                                         wins.per.map$blue.team)*100, 2)
wins.per.map.percentage$blue.team.win.percent <- round(wins.per.map$blue.team / (wins.per.map$red.team +
                                                                           wins.per.map$blue.team)*100, 2)
colnames(wins.per.map.percentage) <- c("Map", "Red team victories", "Blue team victories",
                                       "Red team win %", "Blue team win %")
wins.per.map.percentage
```

### Final score differences

Next, we will discuss the gap between the final scores of the matches.

```{r, echo=FALSE}
df$score.difference <- abs(df$red.score - df$blue.score)
score.difference.summary <- summary(df$score.difference)
score.difference.summary
```

The previous table is a summary of the differences between final scores of a match.
It shows that there was a match where one of the teams lost by 9
points while on the other hand, there was another match where the defeated team,
lost by 528 points. The following five histograms shows the distribution of
the final score differences. The first histogram is for the difference across
all the matches and the following four, are per map.


```{r, echo=FALSE}
hist(df$score.difference, prob = TRUE, main = "Gap between final scores (overall)",
     xlab = "Score difference")
rug(df$score.difference)
lines(density(df$score.difference), col="blue", lwd = 2)
lines(density(df$score.difference, adjust = 2), lty="dotted", col="darkgreen",
      lwd = 2)
```

#### Battle of Kyhlo

```{r, echo=FALSE}
hist(filter(df, map == 1)$score.difference, prob = TRUE,
     main = "Gap between final scores (Battle of Kyhlo)", xlab = "Score difference")
rug(filter(df, map == 1)$score.difference)
lines(density(df$score.difference), col="blue", lwd = 2)
lines(density(df$score.difference, adjust = 2), lty="dotted", col="darkgreen", 
      lwd = 2)
```

#### Forest of Niflhel

```{r, echo=FALSE}
hist(filter(df, map == 2)$score.difference, prob = TRUE,
     main = "Gap between final scores (Forest of Niflhel)", xlab = "Score difference")
rug(filter(df, map == 2)$score.difference)
lines(density(df$score.difference), col="blue", lwd = 2)
lines(density(df$score.difference, adjust = 2), lty="dotted", col="darkgreen",
      lwd = 2)
```

#### Legacy of the Foefire

```{r, echo=FALSE}
hist(filter(df, map == 3)$score.difference, prob = TRUE,
     main = "Gap between final scores (Legacy of the Foefire)", xlab = "Score difference")
rug(filter(df, map == 3)$score.difference)
lines(density(df$score.difference), col="blue", lwd = 2)
lines(density(df$score.difference, adjust = 2), lty="dotted", col="darkgreen", lwd = 2)
```

#### Temple of the Silent Storm

```{r, echo=FALSE}
hist(filter(df, map == 4)$score.difference, prob = TRUE,
     main = "Gap between final scores (Temple of the Silent Storm)", xlab = "Score difference")
rug(filter(df, map == 4)$score.difference)
lines(density(df$score.difference), col="blue", lwd = 2)
lines(density(df$score.difference, adjust = 2), lty="dotted", col="darkgreen", lwd = 2)
```

### Wins per map

Now we will present four visualizations. The first one presents the frequency of
victories of both teams across all the maps, the following two present the scores
of both teams and the last one is a scatterplot the final score of each match of
both teams.


```{r,echo=FALSE}
winners_and_score <- melt(wins.per.map, id = "map")
colnames(winners_and_score) <- c("Map", "Team", "Frequency")
ggplot(data=winners_and_score, aes(x = Team, y = Frequency, fill = Team)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ Map) +
  ggtitle("Wins of both teams per map")
```

```{r, echo=FALSE}
scoresdf <- data.frame(match_number = seq(1,119), red.score = df$red.score, 
                       blue.score = df$blue.score, map = df$map)

qplot(match_number, red.score, data = scoresdf, size = red.score, 
      alpha = red.score, colour = "red", main = "Red team's score",
      xlab = "Match number", ylab = "Score") + theme(legend.position = "none") 

qplot(match_number, blue.score, data = scoresdf, size = blue.score, 
      alpha = blue.score, colour = blue.score, main = "Blue team's score",
      xlab = "Match number", ylab = "Score") + theme(legend.position = "none")
```

```{r, echo=FALSE}
qplot(x = df$red.score, y = df$blue.score, geom = "point", alpha = I(0.5),
      size = I(5), main = "Final score of every match", xlab = "Red score",
      ylab = "Blue score")
```

The previous scatterplot shows the relation between the final scores of both teams.
The points on the upper left corner and bottom right represent the matches where 
one of the teams won by a great difference.

A correlation test was done on these values and the result was  
`r cor(df$red.score, df$blue.score)`, which means
that there is some kind of (negative) correlation on the data. To further confirm
this, we calculated the p-value (found after this), which by being so small implies
that there is a strong evidence against the null hypothesis (the null hypothesis
is that the correlation is 0).

This table presents correlation, p-value and other statistics.

```{r, echo=FALSE}
cor.test(df$red.score, df$blue.score)
```

The next topic to discuss is the scores of the matches. Here, we will take a look at how these scores are distributed and show some statistics about them, accompanied by several plots.

### Final scores of the winning team

In this section we will take a look at the final scores of the winners of all the 
119 matches recorded in this dataset. Before continuing, we would like to mention
that a PvP match in Guild Wars 2 ends when one of the teams reaches a score 
greater or equal to 500.

```{r, echo=FALSE}
# Create a vector with the winner score of each match
winner.score <- pmax(df$red.score, df$blue.score)
summary(winner.score)
```

As expected, the final scores does not vary that much (most of them are close to 500) 
with some them, way over 500 as is the case of the max value of 624; the standard 
deviation of the scores is `r sd(winner.score)`. The reason regarding the score 
of 624 is because one of the maps (Legacy of the Foefire), has an objective that 
gives 150 points to a team.

###Losers score

Unlike the winner's score, the final score of the loser team could be anything 
from 0 (if you are really bad) up to 499 (in most cases).

The following list shows the scores of the loser's team.

```{r, echo=FALSE}
# Create a vector with the score of the defeated team from each match
loser_score <- pmin(df$red.score, df$blue.score)
loser_score
```

The scores are more varied that the winner scores.

The next table shows a summary of this scores, followed by the standard deviation.

```{r, echo=FALSE}
summary(loser_score)
print(sd(loser_score))
```

The summary table shows that during one game, the team who lost, just obtained 
10 points (ouch!), making this the minimum value across the whole dataset, while 
the average was 328.2 points and the max was 491 (so close to winning). We also
calculated the standard deviation, which is `r print(sd(loser_score))`, a value 
greater than the standard deviation of the winner's score, which was 
`r print(sd(winner.score))`. The following two figures shows these results via 
a histogram (that includes a density line) and a boxplot.

```{r, echo=FALSE}
hist(loser_score, prob=TRUE, col = "grey")
rug(loser_score)
lines(density(loser_score), col = "blue", lwd=2)
lines(density(loser_score, adjust=2), lty = "dotted", col = "darkgreen", lwd = 2)
```

The histogram shows that the score of the defeated team during most matches was 
in the range of 350 to 400 and that there were a few games where the score of t
he defeated team was less than 100.

```{r, echo=FALSE}
boxplot(loser_score, main = "Scores of the defeated team", ylab = "Score")
boxplot.stats(loser_score)$out
```

The previous figure, a boxplot, shows the distribution of the scores. The box 
goes from the first quartile (Q1), which is `r boxplot.stats(loser_score)$stats[2]`
to the third quartile (Q3), `r boxplot.stats(loser_score)$stats[4]`. The line 
in the middle shows the median, which is `r boxplot.stats(loser_score)$stats[3]` 
and the top and lower whiskers shows the maximum value (excluding the outliers),
which is `r boxplot.stats(loser_score)$stats[5]` and the minimum value (also 
excluding the outliers), which for this dataset is `r boxplot.stats(loser_score)$stats[1]`. 
The two dots at the bottom of the plot are the outliers. The values of these two
outliers are 10 and 13 points.

The next table shows a summary of the boxplot.

```{r, echo=FALSE}
boxplot.stats(loser_score)$stats
```

The values represent the minimum, first quartile, median, third quartile and 
maximum.

# Composition of the teams
***

The last part of this analysis is about the composition of the teams. Here, we
studied the structure of the teams while answering several questions on the way.
This section will follow a similar structure to the previous ones. We will start 
with a summary and then a detailed analysis about certain topics. Besides this, 
this section introduces several machine learning algorithms that will be used to 
try to predict the outcome of the matches based on the composition of the team 
and the map where the match was played.

As mentioned before, our dataset is made of 119 matches. For this reason, there
are 238 observations of team composition.

### Summary of team composition

This section will start with a summary of the structure of the teams. We will
present the composition of the teams in terms of professions or classes. In
addition to this, we will show how many teams were incomplete.

The first question we will answer is: What is the frequency and percentage of each 
professions across the dataset?

```{r, echo=FALSE}
# Create a dataset with all the different teams
team.composition <- data.frame(class.1 = numeric(0), class.2 = numeric(0), 
                               class.3 = numeric(0), class.4 = numeric(0), 
                               class.5 = numeric(0))
red.team.composition <- df[5:9]
blue.team.composition <- df[10:14]
colnames(red.team.composition) <- c("class.1", "class.2", "class.3", "class.4", 
                                    "class.5")
colnames(blue.team.composition) <- c("class.1", "class.2", "class.3", "class.4",
                                     "class.5")
team.composition <- rbind(team.composition, red.team.composition, blue.team.composition)
```


#### What is the frequency and percentage of each professions across the dataset?

```{r, echo=FALSE}
frequency.percentage.classes <- sapply(1:8, function(class.number) 
  sum(sapply(team.composition, function(x) sum(x == class.number))))
frequency.percentage.classes <- as.data.frame(frequency.percentage.classes)
rownames(frequency.percentage.classes) <- c("Elementalist", "Engineer", "Guardian", "Mesmer",
                                            "Necromancer", "Ranger", "Thief", "Warrior")
colnames(frequency.percentage.classes) <- c("Frequency")
frequency.percentage.classes$Percentage <- 
  round(frequency.percentage.classes$Frequency / (sum(frequency.percentage.classes$Frequency) + 17) * 100, 2)  # 17 missing players
frequency.percentage.classes
```

We can see that the most common class across the matches was the Ranger, with a
frequency of 299 (25.13%), however, the reason why this number much larger than
the other frequencies is the fact that all the matches we did was using a Ranger,
so every single match had at least one Ranger. On the other end, there is the
Engineer, with a frequency of 70 (5.88%).

If the percentages are summed, the result is 
`r round(sum(frequency.percentage.classes$Percentage), 2)`, instead of 100%. This
bring us to other next question: What is the amount of missing players?

#### What is the amount of missing players?

```{r, echo=FALSE}
frequency.percentage.missing.players <- sum(sapply(team.composition, function(x) sum(x == 0)))
frequency.percentage.missing.players <- as.data.frame(frequency.percentage.missing.players)
rownames(frequency.percentage.missing.players) <- c("Missing players")
colnames(frequency.percentage.missing.players) <- c("Frequency")
frequency.percentage.missing.players$Percentage <- round(frequency.percentage.missing.players$Frequency/(sum(frequency.percentage.classes$Frequency) + 17) * 100, 2) # 17 missing players.
frequency.percentage.missing.players
```

17 players were missing. We do not know if these players were missing since the
beginning of the game (for some reason sometimes teams can contain less than 5 players),
or if they disconnected. 

#### Is there a repeated team composition?

In this question we looked for those team compositions that appears more than once.

```{r, echo=FALSE}
team.composition.reorder <- data.frame(class.1 = numeric(0), class.2 = numeric(0), 
                               class.3 = numeric(0), class.4 = numeric(0), 
                               class.5 = numeric(0))
team.composition.reorder <- as.data.frame(t(apply(team.composition, 1 , sort)),
                                          stringsAsFactors = FALSE)
colnames(team.composition.reorder) <- colnames(team.composition)
frequent.teams <- ddply(team.composition.reorder ,names(team.composition.reorder), summarize, 
                        Frequency = length(class.1))

frequent.teams <- frequent.teams[frequent.teams$Frequency > 1,]
frequent.teams[1:5][frequent.teams[1:5] == 1] <- "Elementalist"
frequent.teams[1:5][frequent.teams[1:5] == 2] <- "Engineer"
frequent.teams[1:5][frequent.teams[1:5] == 3] <- "Guardian"
frequent.teams[1:5][frequent.teams[1:5] == 4] <- "Mesmer"
frequent.teams[1:5][frequent.teams[1:5] == 5] <- "Necromancer"
frequent.teams[1:5][frequent.teams[1:5] == 6] <- "Ranger"
frequent.teams[1:5][frequent.teams[1:5] == 7] <- "Thief"
frequent.teams[1:5][frequent.teams[1:5] == 8] <- "Warrior"

arrange(frequent.teams, desc(Frequency))
```

As the table shows, there were `r nrow(frequent.teams)` team configurations that
were repeated more.
than once.

#### What is the outcome of the match when a team is made of 3 or more of the same profession?

```{r, echo=FALSE}
# Find matches that had at least 3 of the same professions
three.or.more <- team.composition[apply(team.composition, 1, function(X) any(table(X) >= 3)),]

scores.for.red <- df$winner  # 1 means red lose, 0 means red won (0 means victory, 1 defeat)
scores.for.blue <- sapply(df$winner, function(x) if (x == 1) { 0 } else { 1 })
scores.for.red <- as.data.frame(scores.for.red)
scores.for.blue <- as.data.frame(scores.for.blue)
colnames(scores.for.red) <- c("Result")
colnames(scores.for.blue) <- c("Result")
team.composition.result <- data.frame(result = numeric(0))
team.composition.result <- rbind(team.composition.result, scores.for.red, scores.for.blue)

# Get the result of the particular matches using the indexes of three.or.more
three.or.more <- cbind(three.or.more, 
                       result = team.composition.result[as.numeric(rownames(three.or.more)),])

```

In the dataset there are `r nrow(three.or.more)` teams that has at least 3
characters playing the same class. Of those `r nrow(three.or.more)` teams,
`r nrow(three.or.more[three.or.more$result == 0,])` won the matches.

These are the composition of those teams.

```{r, echo=FALSE}
three.or.more.victory <- three.or.more[three.or.more$result == 0,]
three.or.more.victory[three.or.more.victory == 1] <- "Elementalist"
three.or.more.victory[three.or.more.victory == 2] <- "Engineer"
three.or.more.victory[three.or.more.victory == 3] <- "Guardian"
three.or.more.victory[three.or.more.victory == 4] <- "Mesmer"
three.or.more.victory[three.or.more.victory == 5] <- "Necromancer"
three.or.more.victory[three.or.more.victory == 6] <- "Ranger"
three.or.more.victory[three.or.more.victory == 7] <- "Thief"
three.or.more.victory[three.or.more.victory == 8] <- "Warrior"
three.or.more.victory
```


#### Is it possible to predict the outcome of a match based on the structure of the team?

```{r, echo=FALSE}
team.result <- cbind(team.composition, team.composition.result)
colnames(team.result)[6] <- "result"
team.result$result[team.result$result == 0 ] <- "won"
team.result$result[team.result$result == 1 ] <- "lost"
team.result$result <- as.factor(team.result$result)
```

The last section of this analysis introduces two classification models, naive
Bayes classifier and random forest. These two models will learn from our data 
with the purpose of classifying the outcome of a match in either 'victory' or 
'defeat', based on the composition of the team.

The basic idea behind these classifiers could be divided in two parts. During
the first part, often called the training part, the model is feeded with data that
already have the outcome of the match. In other words, we are training the model.
The table below shows an example of an observation that will be used for training
the model. This observation have the five members of the team (3 rangers, 
presented with the number 6 and two quitters, presented as 0), and
the result, which is 'lost'.

```{r, echo=FALSE}
team.result[1,]
```

During the second part, after the training is completed, we will send differerent
team compositions to the model and it will output the outcome of the match based
on what it already learned. In other words, we will test the model using a completely
new dataset.

The dataset will be split 70% for training and 30% for testing.

##### Random Forest

The following table is a direct output that has information about the model and
the results.

```{r, echo=FALSE}
# Create the train and test dataset
index <- sample(2, nrow(team.result), replace=TRUE, prob=c(0.8, 0.2))
train.data <- team.result[index == 1, ]
test.data <- team.result[index == 2, ]

# Create the random forest model and train it.
rf <- randomForest(result ~ ., data = train.data, ntree = 70, proximity = TRUE)
confusion.matrix.rf <- table(predict(rf), train.data$result)
classification.error <- round((confusion.matrix.rf[2, 1] + confusion.matrix.rf[1, 2]) /
                           sum(confusion.matrix.rf) * 100, 2)
rf

# Prediction using the test data
pred <- predict(rf, test.data)
pred.confusion.matrix <- table(pred, test.data$result)
test.classification.error <- round((pred.confusion.matrix[2, 1] + pred.confusion.matrix[1, 2]) /
                                     sum(pred.confusion.matrix) * 100, 2)
```

The random forest model reports an error rate of `r classification.error`% during
the training phase, in other words `r classification.error`% of the 
classifications performed during training, were incorrect. More details about 
this can be found at the confusion matrix at the bottom of the table.

After the training phase, we tested the model using the test dataset. The
classification error obtained in this phase is `r test.classification.error`%.
This is the confusion matrix for the test dataset:

```{r, echo=FALSE}                           
#pred.confusion.matrix
pred.confusion.matrix <- cbind(pred.confusion.matrix, 
                               c(pred.confusion.matrix[1, 2] / 
                                   (pred.confusion.matrix[1,1] + pred.confusion.matrix[1,2]), 
                                 pred.confusion.matrix[2, 1] / 
                                   (pred.confusion.matrix[2, 1] + pred.confusion.matrix[2, 2])))
colnames(pred.confusion.matrix) <- c("lost", "won", "class.error")
pred.confusion.matrix
```

##### Naive Bayes classifier

We will use the same training and testing dataset with a different model, a naive
Bayes classifier. This is the confusion matrix and classification error for
this model.


```{r, echo=FALSE}
# Create and train the model
bayes.classifier <- naiveBayes(result ~ ., data = train.data)
train.data <- as.data.frame(cbind(class.1 = as.numeric(train.data$class.1), 
                                  class.2 = as.numeric(train.data$class.2),
                                  class.3 = as.numeric(train.data$class.3), 
                                  class.4 = as.numeric(train.data$class.4),
                                  class.5 = as.numeric(train.data$class.5), 
                                  result = as.numeric(train.data$result)))
pred <- predict(bayes.classifier, train.data)

# Create confusion matrix of the model
bayes.training.confusion.matrix <- table(pred, train.data$result)
bayes.training.confusion.matrix <- 
  cbind(bayes.training.confusion.matrix, 
        c(bayes.training.confusion.matrix[1, 2] /
            (bayes.training.confusion.matrix[1, 1] + bayes.training.confusion.matrix[1, 2]),
          bayes.training.confusion.matrix[2, 1] /
            (bayes.training.confusion.matrix[2, 1] + bayes.training.confusion.matrix[2, 2])))                                       
colnames(bayes.training.confusion.matrix) <- c("lost", "won", "class.error")
bayes.training.confusion.matrix

#Calculate the overall classification error
bayes.training.classification.error <- round((bayes.training.confusion.matrix[2, 1] +
                                                bayes.training.confusion.matrix[1, 2]) /
                                           (bayes.training.confusion.matrix[1,1] +
                                              bayes.training.confusion.matrix[1,2] +
                                              bayes.training.confusion.matrix[2,1] +
                                              bayes.training.confusion.matrix[2,2])
                                         * 100, 2)
                           
```

The naive Bayes model's classification error found during training is
`r bayes.training.classification.error`%.

```{r, echo=FALSE}
# Apply the testing dataset on the model
test.data <- as.data.frame(cbind(class.1 = as.numeric(test.data$class.1), 
                                 class.2 = as.numeric(test.data$class.2),
                                 class.3 = as.numeric(test.data$class.3), 
                                 class.4 = as.numeric(test.data$class.4),
                                 class.5 = as.numeric(test.data$class.5), 
                                 result = as.numeric(test.data$result)))
bayes.prediction <- predict(bayes.classifier, test.data)

# Build the confusion matrix for the testing dataset
bayes.test.confusion.matrix <- table(bayes.prediction, test.data$result)
bayes.test.confusion.matrix <- 
  cbind(bayes.test.confusion.matrix, 
        c(bayes.test.confusion.matrix[1, 2] /
            (bayes.test.confusion.matrix[1, 1] + bayes.test.confusion.matrix[1, 2]),
          bayes.test.confusion.matrix[2, 1] /
            (bayes.test.confusion.matrix[2, 1] + bayes.test.confusion.matrix[2, 2])))
colnames(bayes.test.confusion.matrix) <- c("lost", "won", "class.error")
bayes.test.confusion.matrix

#Calculate the overall classification error
bayes.test.classification.error <- round((bayes.test.confusion.matrix[2, 1] +
                                                bayes.test.confusion.matrix[1, 2]) /
                                           (bayes.test.confusion.matrix[1,1] +
                                              bayes.test.confusion.matrix[1,2] +
                                              bayes.test.confusion.matrix[2,1] +
                                              bayes.test.confusion.matrix[2,2])
                                         * 100, 2)
```

The classification error reported for the testing set on the naive Bayes model is
`r bayes.test.classification.error`%.

##### Conclusion

By looking at the classification errors from both models, we concluded that
the random forest model performed better than the naive Bayes with a training
classification error of `r classification.error`% and a testing classification
error of `r test.classification.error`% in comparison to
`r bayes.training.classification.error`% and `r bayes.test.classification.error`%.

However, our answer to the question "is it possible to predict the outcome of a 
match based on the structure of the team?", would be no. A classification error
of around 50% means that the chance a team has of winning is of 50%, which is
logical in this type of game since there are 2 teams in one match.

To conclude, we would like to repeat that we had a very small data sample of just
119 matches (238 teams), so with more data, the classification error could improve
(or not).


## Future work
***

* More data! If ArenaNet releases an API with record of the matches, we would
repeat this analysis using that data.
* The data used for this analysis was from matches done in around four weeks.
Since then, the metagame and more aspects the game (skills and traits), changes,
so we could repeat this analysis in another period (probably after the release
of the expansion Heart of Thorns).
* Run the models used for prediction using another framework.

## Notes and disclaimers
***

* If you spot a typo, or any error in the analysis, feel free to notify me.
* This work is licensed under a [Creative Commons Attribution-NonCommercial 4.0 International License](http://creativecommons.org/licenses/by-nc/4.0/). In other words, feel 
free to use it, share it, edit it for non-commercial purposes and please, give credit.
* ©2010–2015 ArenaNet, LLC. All rights reserved. Guild Wars, Guild Wars 2, Guild Wars 2: Heart of Thorns, ArenaNet and NCSOFT are trademarks or registered trademarks of NCSOFT Corporation. All other trademarks are the property of their respective owners.


